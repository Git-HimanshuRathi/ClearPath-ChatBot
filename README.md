# Clearpath RAG Chatbot

ðŸš€ **Live Demo â†’ [http://51.20.86.17](http://51.20.86.17)** (Deployed on AWS)

A **Retrieval-Augmented Generation (RAG)** customer support chatbot for **Clearpath**, a fictional SaaS project management platform.

Built with **FastAPI**, **Groq LLMs**, **FAISS**, and a **React/Vite** frontend styled like ChatGPT.

![Deployed](https://img.shields.io/badge/Deployed-AWS%20EC2-FF9900?logo=amazonaws) ![Status](https://img.shields.io/badge/Status-Live-brightgreen)

---

## How It Works

```
User Query â†’ RAG Retrieval â†’ Model Router â†’ Groq LLM â†’ Evaluator â†’ Response
                  â†‘                                          â†“
            FAISS Index                                Quality Checks
         (30 Clearpath docs)                      (hallucination, refusal,
                                                   unsourced pricing)
```

### Three-Layer Pipeline

| Layer       | Component    | Description                                                                              |
| ----------- | ------------ | ---------------------------------------------------------------------------------------- |
| **Layer 1** | RAG Pipeline | PDF ingestion â†’ 500-token chunking â†’ FAISS vector search â†’ top-5 retrieval               |
| **Layer 2** | Model Router | 9-signal scoring classifier routes to 8B (simple) or 70B (complex)                       |
| **Layer 3** | Evaluator    | 5 checks: no-context, refusal, keyword hallucination, context-overlap, unsourced pricing |

---

## ðŸŒ Live Demo

> **[http://51.20.86.17](http://51.20.86.17)**

---

## Local Development

<details>
<summary>Click to expand setup instructions</summary>

```bash
# 1. Setup
python3 -m venv venv && source venv/bin/activate
pip install -r backend/requirements.txt
echo "GROQ_API_KEY=your_key" > .env

# 2. Start backend
cd backend && uvicorn main:app --port 8000 --reload

# 3. Start frontend (new terminal)
cd frontend && npm install && npm run dev
```

</details>

---

## Groq Models

| Classification | Model                     | Use Case                                           |
| -------------- | ------------------------- | -------------------------------------------------- |
| Simple         | `llama-3.1-8b-instant`    | Greetings, single-fact lookups, yes/no             |
| Complex        | `llama-3.3-70b-versatile` | Multi-step questions, comparisons, troubleshooting |

### Router â€” 9 Scoring Signals

```
complex_score = 0
+2  word count â‰¥ 20           (long query)
+2  reasoning keywords         (explain, compare, why, how, recommend...)
+1  comparison patterns        (vs, versus, "difference between", "which plan")
+1  complaint/issue keywords   (not working, broken, error, bug, crash...)
+1  multiple question marks    (multi-part question)
+1  multi-sentence (â‰¥ 3)       (complex context)
-2  word count < 8             (short = simple)
-2  greeting detected          (hi, hello, hey)
-1  yes/no pattern             (yes, no, sure, okay)

â†’ "complex" if score â‰¥ 2, else "simple"
```

### Evaluator â€” 5 Quality Checks

| #   | Check                   | Trigger                                               | Flag                      |
| --- | ----------------------- | ----------------------------------------------------- | ------------------------- |
| 1   | No context              | Zero relevant chunks retrieved                        | `no_context`              |
| 2   | Refusal                 | Response contains "I don't know", "I cannot", etc.    | `refusal_detected`        |
| 3   | Hallucination (keyword) | Out-of-domain terms: blockchain, quantum, NFT...      | `hallucination_keyword`   |
| 4   | Hallucination (overlap) | Response content diverges from context (<30% overlap) | `potential_hallucination` |
| 5   | Unsourced pricing       | Dollar amounts in response not found in source docs   | `unsourced_pricing`       |

When flagged â†’ UI shows: **"âš  Low confidence â€” please verify with support"**

---

## Eval Harness

Run the built-in evaluation suite to validate routing, retrieval, and evaluator behavior:

```bash
cd backend
python eval_harness.py
```

**14 test cases** covering:

- **Routing (5):** Greetings â†’ simple, reasoning â†’ complex, complaints â†’ complex, yes/no â†’ simple
- **Retrieval (3):** Keyboard shortcuts doc retrieved, pricing doc retrieved, off-topic â†’ no results
- **Evaluator (3):** Blockchain â†’ hallucination flag, quantum â†’ hallucination, off-topic â†’ low confidence
- **End-to-end (2):** Integration answer includes "slack", password answer includes "password"

Results saved to `eval_results.json` with per-test pass/fail, classification, model, confidence, and flags.

---

## Project Structure

```
clearpath-chatbot/
â”œâ”€â”€ .env                         # GROQ_API_KEY (not committed)
â”œâ”€â”€ README.md
â”œâ”€â”€ SETUP.md                     # Setup instructions
â”œâ”€â”€ written_answers.md           # Q1-Q4 written answers
â”œâ”€â”€ docs/                        # 30 Clearpath PDF documents
â”œâ”€â”€ data/                        # Auto-generated FAISS index + chunks (gitignored)
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                  # FastAPI app (/query, /chat, /chat/stream)
â”‚   â”œâ”€â”€ eval_harness.py          # 14-test evaluation suite
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ ingest.py            # PDF text extraction (PyPDF2)
â”‚   â”‚   â”œâ”€â”€ chunk.py             # 500-token chunking, 100-token overlap
â”‚   â”‚   â”œâ”€â”€ embed.py             # all-MiniLM-L6-v2 embeddings + FAISS
â”‚   â”‚   â””â”€â”€ retrieve.py          # Top-5 retrieval with similarity threshold
â”‚   â”œâ”€â”€ router/router.py         # 9-signal scoring classifier
â”‚   â”œâ”€â”€ evaluator/evaluator.py   # 5-check response quality evaluator
â”‚   â”œâ”€â”€ llm/groq_client.py       # Groq API wrapper (standard + streaming)
â”‚   â””â”€â”€ logs/logger.py           # Structured JSON logging
â””â”€â”€ frontend/
    â”œâ”€â”€ index.html
    â”œâ”€â”€ vite.config.js
    â””â”€â”€ src/
        â”œâ”€â”€ App.jsx              # Multi-conversation state + SSE streaming
        â”œâ”€â”€ index.css            # Tailwind v4 + dark theme tokens
        â””â”€â”€ components/
            â”œâ”€â”€ Sidebar.jsx      # Chat list sidebar
            â”œâ”€â”€ MainLayout.jsx   # Layout composition
            â”œâ”€â”€ Header.jsx       # Model indicator + debug toggle
            â”œâ”€â”€ ChatWindow.jsx   # Centered messages + suggestion chips
            â”œâ”€â”€ ChatMessage.jsx  # Markdown rendering + confidence labels
            â”œâ”€â”€ ChatInput.jsx    # Auto-resize textarea
            â””â”€â”€ DebugPanel.jsx   # Model, tokens, flags, sources
```

---

## API Endpoints

### `POST /query` â€” Assignment API Contract

```json
// Request
{ "question": "What are the pricing plans?", "conversation_id": "optional" }

// Response
{
  "answer": "...",
  "metadata": {
    "model_used": "llama-3.3-70b-versatile",
    "classification": "complex",
    "tokens": { "input": 1234, "output": 156 },
    "latency_ms": 847,
    "chunks_retrieved": 5,
    "evaluator_flags": []
  },
  "sources": [{ "document": "14_Pricing_Sheet_2024.pdf", "relevance_score": 0.92 }],
  "conversation_id": "conv_abc123"
}
```

### `POST /chat/stream` â€” SSE Streaming (Frontend)

### `GET /health` â€” Health Check

---

## Design Decisions

1. **FAISS** â€” Zero external dependencies, fast for 30 documents, persisted to disk
2. **9-signal scoring router** â€” Deterministic, zero-latency, transparent signal breakdown logged per request
3. **5-check evaluator** â€” Comprehensive: no-context, refusal, keyword hallucination, context-overlap, and domain-specific unsourced pricing
4. **Cosine similarity threshold (0.25)** â€” Permissive to avoid zero-result queries; evaluator catches worst cases
5. **Strict grounding prompt** â€” LLM refuses rather than hallucinating
6. **Eval harness** â€” 14 automated test cases for regression testing

---

## Bonus Challenges

| Challenge           | Status                                                |
| ------------------- | ----------------------------------------------------- |
| Conversation memory | âœ… Last 3 turns per session                           |
| Streaming           | âœ… SSE token-by-token with fallback                   |
| Eval harness        | âœ… 14 test cases (routing, retrieval, evaluator, E2E) |

---

## Known Limitations

1. Similarity threshold (0.25) can retrieve marginally relevant chunks
2. No temporal awareness â€” treats all documents as equally current
3. No cross-encoder re-ranking step after FAISS retrieval
